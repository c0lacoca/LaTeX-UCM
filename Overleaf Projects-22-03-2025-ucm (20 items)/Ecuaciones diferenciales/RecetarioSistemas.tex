
\chapter{Tipos y métodos de resolución de sistemas de ecuaciones diferenciales}
\section{Sistemas de ecuaciones lineales}
Como se ha visto en teoría, un sistema de ecuaciones lineal $Y'=AY+B$, tiene como solución $Y_g=Y_p+Y_h$ donde $Y_h=e^{tA}C$ y luego $Y_p=C(t)=\int W^{-1}\cdot B \: dt$ (con $W:=\Big( Y_1 \:  | \:  Y_2 \: | \: \cdots \: | \:  Y_n \Big) )$ por el método de variación de las constantes.

\subsection{Cálculo de exponenciales}
Recordemos que 
$$e^A=\sum_{k=0}^{\infty} \dfrac{A^k}{k!}$$
luego en general, no podremos hallar la exponencial, en cambio, podemos simplificar algunos casos.

\underline{Caso diagonal}

$$A=\begin{pmatrix}
    d_1 \\
    & \ddots \\
    &&d_n
\end{pmatrix} \; \Rightarrow \; e^A=\begin{pmatrix}
    e^{d_1} \\
    & \ddots \\
    &&e^{d_n}
\end{pmatrix}$$

\underline{Caso nilpotente}

Es decir, $ \exists \: j , A^{j+1}=0$, entonces
$$e^A=\sum_{k=0}^j \dfrac{A^k}{k!}$$

\underline{Si $A_1$ y $A_2$ conmutan entre sí:}
$$e^{A_1+A_2}=e^{A_1}\cdot e^{A_2}$$

\underline{Cambio de base}

La exponencial es compatible con el cambio de base, i.e.
$$B \cdot e^A \cdot B^{-1}=e^{B \: A \: B^{-1}}$$

\underline{Reducción a la forma canónica de Jordan}

Existe un cambio de base tal que $A=BJB^{-1}$ donde $J=$ Forma canónica de Jordan verifica que $J=D+N$ donde $D$ es diagonal y $N$ es nilpotente. 

$N$ es nilpotente por tener elementos no nulos solo a un lado de la diagonal. Cada vez que se eleve al cuadrado, los 1's se desplazan a la izquierda, así pues en $n$ pasos como máximo, obtenemos la matriz nula.

Así pues
$$e^A=e^{BJB^{-1}}=B e^J B^{-1}=B e^D e^N B^{-1}$$

\begin{eje}
    Sea la matriz $$A=\begin{pmatrix}
        -5 & -1\\
        1 & -3
    \end{pmatrix}=\begin{pmatrix}
        -1 & 1 \\
        1 & 0
    \end{pmatrix}\begin{pmatrix}
        -4 & 1 \\
        0 & -4
    \end{pmatrix}\begin{pmatrix}
        0 & 1 \\
        1 & 1
    \end{pmatrix}=BJB^{-1}$$
    $$e^J=e^D e^N=\begin{pmatrix}
        e^{-4} & 0  \\
        0 & e^{-4}
    \end{pmatrix}(I+N)=\begin{pmatrix}
        e^{-4} & 0 \\
        0 & e^{-4}
    \end{pmatrix}\begin{pmatrix}
        1 & 1 \\
        0 & 1
    \end{pmatrix}=e^{-4}\begin{pmatrix}
        1 & 1 \\
        0 & 1
    \end{pmatrix}$$
    para resolver un sistema, introducimos $t$:
    $$e^{tA}=\begin{pmatrix}
        -1 & 1 \\
        1 & 0
    \end{pmatrix}t\begin{pmatrix}
        -4 & 1 \\
        0 & -4
    \end{pmatrix}\begin{pmatrix}
        0 & 1 \\
        1 & 1
    \end{pmatrix}$$
    $$e^{tD}=\begin{pmatrix}
        e^{-4t} & 0 \\
        0 & e^{-4t}
    \end{pmatrix} \qquad e^{tN}=I+tN=\begin{pmatrix}
        1 & t \\
        0 & 1
    \end{pmatrix}$$
    $$e^{tA}=\begin{pmatrix}
        -1 & 1 \\
        1 & 0
    \end{pmatrix}e^{-4t}\begin{pmatrix}
        1 & t \\
        0 & 1
    \end{pmatrix}\begin{pmatrix}
        0 & 1 \\
        1 & 1
    \end{pmatrix}$$
\end{eje}
\begin{ejer}
    \textbf{30.b)}
    $$\left\{ \begin{array}{l}
         \dfrac{dx}{dt}=-5x-y  \\
         \vspace{-5mm} \\
         \dfrac{dy}{dt}=x-3y
    \end{array} \right. \rightsquigarrow \begin{pmatrix}
        \nicefrac{dx}{dt} \\
        \nicefrac{dy}{dt}        
    \end{pmatrix}=\begin{pmatrix}
        -5 & -1\\
        1 & -3
    \end{pmatrix}\begin{pmatrix}
        x\\
        y
    \end{pmatrix}$$
\end{ejer}
\begin{sol}
    Las soluciones son 
    $$\begin{pmatrix}
        x \\
        y
    \end{pmatrix}=e^{tA}\begin{pmatrix}
        c_1 \\
        c_2
    \end{pmatrix}=\begin{pmatrix}
        -1 & 1 \\
        1 & 0
    \end{pmatrix}e^{-4t}\begin{pmatrix}
        1 & t \\
        0 & 1
    \end{pmatrix}\begin{pmatrix}
        0 & 1 \\
        1 & 1
    \end{pmatrix}\begin{pmatrix}
        c_1 \\
        c_2
    \end{pmatrix}=e^{-4t}\begin{pmatrix}
        -1 & -t+1 \\
        0 & t
    \end{pmatrix}\begin{pmatrix}
        0 & 1 \\
        1 & 1
    \end{pmatrix}\begin{pmatrix}
        c_1 \\
        c_2
    \end{pmatrix}=$$
    $$=e^{-4t}\begin{pmatrix}
        -t+1 & -t \\
        t & t+1
    \end{pmatrix}\begin{pmatrix}
        c_1 \\
        c_2
    \end{pmatrix}=e^{-4t}\begin{pmatrix}
        c_1(-t+1)-c_2t \\
        c_1t+c_2(t+1)
    \end{pmatrix}$$
    es decir, 
    $$\left\{\begin{array}{l}
         x(t)= e^{-4t}(c_1(-t+1)-c_2t)  \\
         y(t) =e^{-4t}( c_1t+c_2(t+1))
    \end{array} \right.$$
\end{sol}
 \label{30f}
\begin{ejer}
    \textbf{30.f)} Calcular la solución homogénea de
    $$\left\{  \begin{array}{l}
         \dfrac{dx}{dt}=y  \\
         \vspace{-5mm} \\
         \dfrac{dy}{dt}=-x
    \end{array}\right. \longleadsto{1} \begin{pmatrix}
        \nicefrac{dx}{dt} \\
        \nicefrac{dy}{dt}        
    \end{pmatrix}=\begin{pmatrix}
        0 & 1\\
        -1 & 0
    \end{pmatrix}\begin{pmatrix}
        x\\
        y
    \end{pmatrix}$$
\end{ejer}
\begin{sol}
    Veamos como puede que, aunque la matriz diagonalice con v.p. complejos y con la matriz de cambio de base también compleja, al final todo se mantiene en los reales, ya que $e^A$ tiene valores reales:
    $$\begin{pmatrix}
        0 & 1\\
        -1 & 0
    \end{pmatrix}=\begin{pmatrix}
        i & -i\\
        1 & 1
    \end{pmatrix}\begin{pmatrix}
        -i & 0\\
        0 & i
    \end{pmatrix}\dfrac{1}{2}\begin{pmatrix}
        -i & 1\\
        i & 1
    \end{pmatrix}$$
    $$e^{tD}=\begin{pmatrix}
        e^{-it} & 0\\
        0 & e^{it}
    \end{pmatrix}$$
    $$e^{tA}=\begin{pmatrix}
        i & -i\\
        1 & 1
    \end{pmatrix}\begin{pmatrix}
        e^{-it} & 0\\
        0 & e^{it}
    \end{pmatrix}\dfrac{1}{2}\begin{pmatrix}
        -i & 1\\
        i & 1
    \end{pmatrix}=\dfrac{1}{2}\begin{pmatrix}
        ie^{-it} & -ie^{it}\\
        e^{-it} & e^{it}
    \end{pmatrix}\begin{pmatrix}
        -i & 1\\
        i & 1
    \end{pmatrix}=\dfrac{1}{2}\begin{pmatrix}
        e^{it}e^{-it} & ie^{-it}-ie^{it}\\
        -ie^{-it}+ie^{it} & e^{it}e^{-it}
    \end{pmatrix}=$$
    $$=\begin{pmatrix}
        \cos t & \sin t \\
        -\sin t & \cos t
    \end{pmatrix}$$
    y por tanto, las soluciones son 
    $$\begin{pmatrix}
        x\\
        y
    \end{pmatrix}=\begin{pmatrix}
        \cos t & \sin t \\
        -\sin t & \cos t
    \end{pmatrix}\begin{pmatrix}
        c_1\\
        c_2
    \end{pmatrix}$$
    \begin{obs}
        Dada una matriz dada por una exponencial $e^{tA}$, se verifica que 
        $$e^{-tA}=\left( e^{tA}\right)^{-1}$$
        es decir, cambiar el signo de la $t$, simplifando enormemente los cálculos
    \end{obs}
\end{sol}
\begin{ejer}
    \textbf{30.c)} Dada la matriz
    $$A=\begin{pmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        1 & 0 & 0 
    \end{pmatrix}$$
\end{ejer}
\begin{sol}
    El cambio de base es (siendo $\alpha$ una raíz $n$-ésima dela unidad),
    $$\begin{pmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        1 & 0 & 0 
    \end{pmatrix}=\begin{pmatrix}
        1 & \alpha^2 & \alpha \\
        1 & \alpha & \alpha^2 \\
        1 & 1 & 1 
    \end{pmatrix}\begin{pmatrix}
        1 & & \\
         & \alpha^2 & \\
         &  & \alpha
    \end{pmatrix}\dfrac{1}{3}\begin{pmatrix}
        1 & 1 & 1 \\
        \alpha & \alpha^2 & 1 \\
        \alpha^2 & \alpha & 1 
    \end{pmatrix}$$
    $$e^{tD}=\begin{pmatrix}
         e^t & & \\
         & e^{t\alpha^2} & \\
         &  & e^{t\alpha}
    \end{pmatrix}$$
\end{sol}
\section{Método de variación de las constantes}
\begin{ejer}
    \textbf{30.f)} Terminar el sistema ahora no homogéneo:
    $$\left\{  \begin{array}{l}
         \dfrac{dx}{dt}=y  \\
         \vspace{-5mm} \\
         \dfrac{dy}{dt}=-x+\dfrac{1}{\cos t}
    \end{array}\right.\longleadsto{1} \begin{pmatrix}
        \nicefrac{dx}{dt} \\
        \nicefrac{dy}{dt}        
    \end{pmatrix}=\begin{pmatrix}
        0 & 1\\
        -1 & 0
    \end{pmatrix}\begin{pmatrix}
        x\\
        y
    \end{pmatrix}+\begin{pmatrix}
        0 \\
        \nicefrac{1}{\cos t}
    \end{pmatrix}$$
\end{ejer}
\begin{sol}
    La solución homogénea era
    $$\begin{pmatrix}
        x\\
        y
    \end{pmatrix}=e^{tA}C=\begin{pmatrix}
        \cos t & \sin t \\
        -\sin t & \cos t
    \end{pmatrix}\begin{pmatrix}
        c_1\\
        c_2
    \end{pmatrix}$$
    Y ahora la solución particular
    $$\begin{pmatrix}
        x_p\\
        y_p
    \end{pmatrix}=e^{tA} C(t) \text{ con } e^{tA} C'(t)=\begin{pmatrix}
        0 \\
        \nicefrac{1}{\cos t}
    \end{pmatrix} \; \iff \; C'(t)=e^{-At}\begin{pmatrix}
        0 \\
        \nicefrac{1}{\cos t}
    \end{pmatrix}=\begin{pmatrix}
        \cos t & -\sin t \\
        \sin t & \cos t
    \end{pmatrix}\begin{pmatrix}
        0 \\
        \nicefrac{1}{\cos t}
    \end{pmatrix}$$
    Entonces 
    $$C'(t)=\begin{pmatrix}
        c_1'(t) \\
        c_2'(t)
    \end{pmatrix}=\begin{pmatrix}
        -\nicefrac{\sin t }{\cos t} \\
        1
    \end{pmatrix} \; \iff \; \left\{ \begin{array}{l}
         c_1'(t)= -\dfrac{\sin t }{\cos t}  \\
         c_2'(t)=1
    \end{array} \right. \; \Longrightarrow \; \left\{ \begin{array}{l}
         c_1(t)= \log|{\cos t}|  \\
         c_2(t)=t
    \end{array} \right.$$
    de forma que la solución general es
    $$\left\{ \begin{array}{l}
        x=(c_1+\log|{\cos t}|)\cos t+(c_2+t)\sin t  \\
        y=-(c_1+\log|{\cos t}|)\sin t+(c_2+t)\cos t
    \end{array} \right.$$
\end{sol}
\chapter{Tipos y métodos de resolución de ecuaciones diferenciales ordinarias de orden superior}
\section{Cambio de variables}
En teoría hemos visto la forma ``canónica'' de resolverlos, pero existen métodos específicos, como vemos a continuación:
\begin{ejer}
    \textbf{43.a)} $y^{(5}-\dfrac{1}{x} y^{(4}=0$
\end{ejer}
\begin{sol}
    Sea $z=y^{(4}$, tenemos la ecuación $z'-\dfrac{1}{x}z=0$ cuya solución es $z=cx$, entonces deshaciendo el cambio, 
    $$y^{(4}=c \: x \; \Rightarrow \; y^{(3}=c \dfrac{x^2}{2}+c_2 \; \Rightarrow \; y''=c \dfrac{x^3}{6}+c_2x+c_3 \; \Rightarrow \; y'=c \dfrac{x^4}{24}+c_2 \dfrac{x^2}{2}+c_3x+c_4 \; \Rightarrow \;$$
    $$ y=c \dfrac{x^5}{120} + c_2 \dfrac{x^3}{6}+\dfrac{x^2}{2}+c_4x+c_5 \qquad c_i \in \mathbb R \forall i = \{1, \ldots, 5\}$$
\end{sol}
\section{Métodos aternativos ``e''}
\begin{ejer}
    \textbf{43.b)} $yy''-(y')^2=0$
\end{ejer}
\begin{sol}
    Haciendo $p=y'$, hagamos 
    $$\dfrac{dp}{dy}=\dfrac{\nicefrac{dp}{dx}}{\nicefrac{dy}{dx}}=\dfrac{y''}{p} \; \Rightarrow \; y''=p \dfrac{dp}{dy}$$
y sustituyendo en la ecuación original
$$y \cdot p \dfrac{dp}{dy}-p^2=0  \; \iff \; p\left( y \cdot \dfrac{dp}{dy}-p=0\right) \; \iff \; \left\{ \begin{array}{l}
     p = 0 \; \iff \; y'=0 \; \iff \; y=c  \\
     \vspace{-5mm} \\
     y \dfrac{dp}{dy}-p=0 \; \iff \; \dfrac{dp}{p}-\dfrac{dy}{y}=0 \; \iff \; p=cy  \\
     \qquad \qquad \quad \iff \; y'=cy \; \iff \; \boxed{ \: y=c_2\cdot e^{cx} \: }
\end{array}\right.$$
\end{sol}
\section{Lineales homogéneas con coeficientes constantes}
Son ecuaciones de la forma
$$y^{(n}+a_{n-1}y^{(n-1}+ \: \cdots \: + a_2 y^{(2}+a_1y^{(1}+a_oy=0 \quad a_i \in \mathbb R$$
Definimos el operdor derivar $Df:=f'$ y denotamos a $D \circ D := D^2$, pudiendo definir polinomios como
$$P(D)=\sum_{i=0}^{k} \lambda_i D_i=\text{ ``Operador en }D\text{''}$$
Con este operador, la ecuación de partida es 
$$(D^{n}+a_{n-1}D^{n-1}+ \: \cdots \: + a_2 D^{2}+a_1D+a_o)y=P(D)y=0$$
de forma que es la aplicación 
$$\begin{array}{rcl}
     \mc{C}^{\infty} (\mathbb R) & \overset{P(D)}{\longrightarrow} &  \mc{C}^{\infty} (\mathbb R)   \\
\end{array} \qquad \mathbb R \text{-lineal}$$
y por tanto, resolver la ecuación es calcular el núcleo de $P(D)$.
\subsection{Cálculo del núcleo}
\begin{enumerate}
    \item Si $P(D)=D$, $\ker D=\{ctes.\}=\mathbb R = <1>_{\mathbb R}$
    \item Si $P(D)=D-\alpha$, $\ker D-\alpha =\footnote{\text{\underline{Obs.:}} $\ker D-\alpha \iff (D-\alpha)f=0 \iff Df=\alpha f$}\{ce^{\alpha x}, \: c \in \mathbb R\}=<e^{\alpha x}_{\mathbb R}>$.
    \item Si $P(D)=D^n$, $\ker D^n=\{c_o + c_1x+c_2x^2+\cdots+c_{n-1}x^{n-1}, \: c_i \in \mathbb R\}=<1,x,x^2, \ldots x^{n-1}>_{\mathbb R}$
    \item Si $P(D)=(D-\alpha)^n$, veamos la 

    \underline{Fórmula de conmutación}
    $$D(e^{\beta x}f)=\beta e^{\beta x}f+e^{\beta x}Df=e^{\beta x}(D+\beta)f \qquad D^2(e^{\beta x}f)=e^{\beta x}(D+\beta)^2f $$
    $$\boxed{ P(D)(e^{\beta x}f)=e^{\beta x} P(D+\beta)f }$$

    entonces,
    $$0=(D-\alpha)^nf=(D-\alpha)^n(e^{\alpha x}e^{-\alpha x}f)=e^{\alpha x}(D+\cancel{\alpha}-\cancel{\alpha})(e^{-\alpha x}f) \; \iff \; D^n(e^{-\alpha x}f)=0 $$
    $$\; \iff \; e^{-\alpha x}f \in \ker D^n=<1,x,x^2, \ldots x^{n-1}> \; \iff \; $$
    $$f \in Ker (D-\alpha)^n=<e^{\alpha x}, xe^{\alpha x}, x^2 e^{\alpha x}, \ldots, x^{n-1}e^{\alpha x}>$$
\end{enumerate}
\begin{prop}
    Sean ahora $P(D) \cdot Q(D)$ con $P,Q$ primos entre sí, entonces
    $$\ker(P\cdot Q)=\ker P \oplus \ker Q$$
\end{prop}
\begin{dem}
    Por el lema de Bezout, existen $A(D)$ y $B(D)$ tales que $AP+BQ=1$. Si $f \in \ker P\cdot Q$, entonces 
    $$f=1\cdot f = \underbrace{A(D)P(D)f}_{\in \ker Q(D)}+\underbrace{B(D)Q(D)f}_{\in \ker P(D)}$$
    luego se verifica que $\ker(P\cdot Q)=\ker P + \ker Q$. Definimos ahora $f \in \ker P \cap \ker Q$,
    $$f=\cancelto{0}{A(D)P(D)f}+\cancelto{0}{B(D)Q(D)f}=0$$
\end{dem}
\begin{eje}
    $(D-2)^2(D-\pi)^4y=0$
    $$\ker (D-2)^3 \oplus \ker(D+\pi)^4=<e^{2x},xe^{2x},x^2e^{2x}>\oplus <e^{-\pi x},xe^{-\pi x},x^2e^{-\pi x},x^3e^{-\pi x}>$$
\end{eje}
\subsection{Caso de raíces no reales}
A veces, tenemos factores del tipo
$$\big((D+a)^2+b^2\big)^n$$
lo cual es posible ya que podemos considerar funciones $\mc{C}^{\infty}(\mathbb R, \mathbb C)$, considerando $\mathbb C \simeq \mathbb R^2$ y derivando componente a componente. Veamos cómo procedemos:
\begin{itemize}
    \item Si $n=1$,
$$(D+a)^2+b^2=(D-(a+bi))(D-(a-bi))$$
calculamos su núcleo
$$\ker_{\mathbb C}((D+a)^2+b^2)=\ker_{\mathbb C} (D-(a+bi)) \oplus\ker_{\mathbb C} (D-(a-bi)) $$
donde
$$\ker_{\mathbb C} (D-(a+bi))=<e^{(a+bi)x}>_{\mathbb C}=<e^{ax}(\cos (bx) + i\sin (bx))>_{\mathbb C}$$
y de la misma forma
$$\ker_{\mathbb C} (D-(a-bi))=<e^{ax}(\cos (bx) - i\sin (bx))>_{\mathbb C}$$
y por tanto
$$\ker_{\mathbb C}((D+a)^2+b^2)=<e^{ax}(\cos (bx) + i\sin (bx)), \; e^{ax}(\cos (bx) - i\sin (bx))>_{\mathbb C}$$
Si las sumamos, y a parte, las restamos, tenemos las soluciones
$$e^{ax}\cos(bx), \: e^{ax} \sin(bx) \in \ker_{\mathbb R}((D+a)^2+b^2)$$
que son 2 (la dimensión del núcleo) y son linealmente independientes, y por tanto base 
$$\ker_{\mathbb R}((D+a)^2+b^2)=<e^{ax}\cos(bx), \: e^{ax} \sin(bx)>_{\mathbb R}$$
\item Sea $n$ genérica
$$\ker_{\mathbb R}((D+a)^2+b^2)^n=<e^{ax}\cos(bx), \: xe^{ax}\cos(bx), x^2 e^{ax}\cos(bx), \ldots, x^{n-1}e^{ax}\cos(bx), $$$$e^{ax} \sin(bx), xe^{ax} \sin(bx) , \ldots , x^{n-1} e^{ax} \sin(bx)>$$
\end{itemize}
\begin{eje}
    $y''+4y=0$
    $$\ker(D^2+4)=<e^{0x}\cos(2x),e^{0x}\sin(2x)>=<\cos(2x),\sin(2x)>$$
\end{eje}
\begin{ejer}
    \textbf{44.b)} $y^{(4}+4y'''+10y''+12y'+5y=0$
\end{ejer}
\begin{sol}
    El polinomio en $D$ asociado es 
    $$P(D)=(D^4+4D^3+10D^2+12D+5)=(D+1)^2((D-1)^2+1^2)$$
    luego 
    $$\ker \: P(D)=\ker (D+1)^2 \oplus \ker((D-1)^2+1^2)=<e^{-x}, xe^{-x}, e^{x}\cos x, e^{x}\sin x>$$
\end{sol}
\section{Lineales completas}
Ahora, con el polinomio asociado, son de la forma 
$$P(D)y=b(x)$$
que al igual que para ecuaciones o sistemas, buscamos una solución particular, para lo cual tenemos los siguientes métodos:
\begin{itemize}
    \item \underline{Método de variación de las constantes}, como hacíamos antes. 
    
    Dadas $y_i$ soluciones de la ecuación homogénea, se verifica que $\displaystyle 
    y=\sum_{i=1}^n c_i y_i $ es solución. Entonces variando las constantes, buscamos que $\displaystyle y=\sum_{i=1}^n c_i(x) y_i $ sea solución de la particular (siendo $c_i(x)$ $n$ funciones desconocidas).

    Ahora bien, si derivamos $y$, buscamos que sus derivadas tengan en lo posible la misma forma que tienen cuando las $c_i$ son constantes, es decir, escojamos $c_i(x)$ de manera tal que 
    $$y'=\sum_{i=1}^n c_i(x) y_i' +\sum_{i=1}^n c_i'(x) y_i =\sum_{i=1}^n c_i(x) y_i' \; \Rightarrow \;  \sum_{i=1}^n c_i'(x) y_i = 0$$
    De la misma forma, buscamos que 
    $$y''=\sum_{i=1}^n c_i(x) y_i'' +\sum_{i=1}^n c_i'(x) y_i'  =\sum_{i=1}^n c_i(x) y_i''  \; \Rightarrow \;  \sum_{i=1}^n c_i'(x) y_i' = 0 $$
    Podemos seguir realizando este proceso hasta la derivada $n-1$ inclusive. En cambio, en la derivada $n$-ésima,
    $$y^{(n}=\sum_{i=1}^n c_i(x) y_i^{(n} +\sum_{i=1}^n c_i'(x) y_i^{(n-1} $$
    no podemos exigir que el segundo término sea 0, ya que tenemos $n$ incógnitas que verifican $n-1$ condiciones, pero también tienen que verificar la ecuación diferencial inicial. 

    Sustituyendo en ella las derivadas, de $y$, 
    $$y^{(n}+a_{n-1}y^{(n-1} + \cdots + a_{1}y' + a_o y = b(x) $$
    $$\sum_{i=1}^n c_i(x) y_i^{(n} +\sum_{i=1}^n c_i'(x) y_i^{(n-1}  + a_{n-1} \sum_{i=1}^n c_i(x) y_i^{(n-1}  + \cdots + a_1 \sum_{i=1}^n c_i(x) y_i' + a_o \sum_{i=1}^n c_i(x) y_i = b(x)  $$
    $$\sum_{i=1}^n c_i'(x) y_i^{(n-1} + \sum_{i=1}^n c_i(x) [\cancel{y_i^{(n} + a_{n-1}  y_i^{(n-1}  + \cdots + a_1  y_i' + a_o  y_i}] ^{(*)} = b(x) $$
    $$\sum_{i=1}^n c_i'(x) y_i^{(n-1}  = b(x)$$
    $^{(*)}$ se cancelan por ser las soluciones de la ecuación homogénea

    De esta forma, tenemos el sistema de ecuaciones para las $c_i'(x)$:
    $$\displaystyle \left\{ \begin{array}{l}
         \displaystyle \sum_{i=1}^n c_i'(x) y_i  = 0  \\
         \displaystyle \sum_{i=1}^n c_i'(x) y_i'  = 0 \\
         \displaystyle \sum_{i=1}^n c_i'(x) y_i''  = 0 \\
         \qquad \vdots \\
         \displaystyle \sum_{i=1}^n c_i'(x) y_i^{(n-2}  = 0 \\
         \displaystyle \sum_{i=1}^n c_i'(x) y_i^{(n-1}  = b(x)
    \end{array} \right.$$
    que además siempre tiene solución, ya que su determinante es el Wronskiano:
    $$|W|=\begin{vmatrix}
        y_0 & y_1 & \cdots & y_{n-1} \\
        y_0' & y_1' & \cdots & y_{n-1}' \\
        y_0'' & y_1'' & \cdots & y_{n-1}'' \\
        \vdots & \vdots & \vdots & \vdots \\
        y_0^{(n-1} & y_1^{(n-1} & \cdots & y_{n-1}^{(n-1} \\
    \end{vmatrix}$$
    y como hemos visto, siempre es invertible, es decir, que su determinante nunca es nulo. Al determinar por tanto todas las $c_i'(x)$ del sistema, basta resolver por cuadraturas, teniendose
    $$c_i=\int \varphi_i(x) + \bar{c}_i$$
    
    \item \underline{Invertir el operador $D$}. Ya que $P(D)y=b(x)$, podemos despejar $y=\dfrac{1}{P(D)} \: b(x)$, en el sentido de:
    $$Dy=b(x) \; \iff \; y=\dfrac{1}{D} \: b(x)=\int b(x) \qquad D^2y=b(x) \: \iff \: y=\dfrac{1}{D^2} \: b(x)=\iint b(x) $$
    \vspace{-5mm}
    $$(D- \alpha)y=b(x) \; \iff \; y=\dfrac{1}{D-\alpha} \: b(x)=\dfrac{1}{D-\alpha} e^{\alpha x}e^{-\alpha x}b(x)=e^{\alpha x} \dfrac{1}{D} e^{-\alpha x} b(x)=\int e^{-\alpha x}b(x)$$
    \vspace{-5mm}
    $$(D-\alpha)^3y=b(x) \; \iff \; y=\dfrac{1}{(D-\alpha)^3} b(x)=e^{\alpha x} \dfrac{1}{D^3}(e^{-\alpha x}b(x))=\iiint e^{-\alpha x} b(x)$$
    \vspace{-5mm}    
    $$D^2(D-\beta)y=b(x) \; \iff \; y=\dfrac{1}{D^2(D-\beta)} \: b(x)=\dfrac{1}{D-\beta} \iint b(x)=e^{\beta x}\int e^{-\beta x} \iint b(x) $$
    y si tenemos solución imaginaria en $\alpha$, no hay problema, ya que debemos igualar un número imaginario a otro real, eliminando la parte imaginaria.
    \item \underline{Método del anulador}. Si $P(D)y=b(x)$, es necesario que $Q(D) \cdot b(x)=0$ y que $P$ y $Q$ sean primos entre sí.

    Por el Lema de Beozut, existen $A, B$ tal que $AP+BQ=1$, luego multiplicando por $b(x)$
    $$P(D) \: A(D) \: b(x)+ B(D) \: \cancel{Q(D) \: b(x)}=b(x)$$
    y la solución particular es $A(D) \cdot b(x)$
\end{itemize}
\begin{eje} Usando el primer método, resolver la ecuación $$y''+y=\dfrac{1}{\cos x}$$
        Por el cálculo del núcleo de $(D^2+1)$, las soluciones de la homogénea son $y=c_1 \sin x + c_2 \cos x$, luego variando $c_1, \: c_2$
        $$y=c_1(x) \sin x + c_2(x) \cos x$$
        las determinamos a partir del sistema
        $$\left\{ \begin{array}{l}
             c_1'(x) \cos x + c_2'(x) \sin x = 0\\
             - c_1'(x) \sin x + c_2'(x) \cos x = \dfrac{1}{\cos x}
        \end{array}\right. \; \Rightarrow \; \left\{ \begin{array}{l}
             c_1'(x) = - \dfrac{c_2'(x) \sin x}{\cos x} \\
             \frac{c_2'(x) \sin^2 x}{\cos x} + c_2'(x) \cos x = \dfrac{1}{\cos x} 
        \end{array}\right. \; \Rightarrow $$
        $$\left\{ \begin{array}{l}
             c_1'(x) = - \dfrac{c_2'(x) \sin x}{\cos x} \\
             c_2'(x)\left( \frac{ \cos x \sin^2 x}{\cos x} +  \cos^2 x \right)= 1
        \end{array}\right. \; \Rightarrow \; \left\{ \begin{array}{l}
             c_1'(x) = - \dfrac{\sin x}{\cos x} \\
             c_2'(x) = 1
        \end{array}\right.$$
        y por tanto las soluciones de la particular son
        $$\left\{ \begin{array}{l}
             \displaystyle c_1(x) = \int \dfrac{-\sin x}{\cos x} dx = \ln|\cos x| + \bar{c}_1 \\
             c_2(x) = \int 1 dx = x+\bar{c}_2
        \end{array}\right.$$
        de forma que la solución de general de la ecuación inicial es 
        $$\boxed{\; y=  \bar{c}_1  \sin x + \bar{c}_2 \cos x + \cos x \ln|\cos x| + x \sin x \; }$$
    \end{eje}
    
\begin{ejer}
    \textbf{45.a)} \textcolor{red}{Usando el segundo método}
\end{ejer}
\begin{ejer}
    \textbf{45.b)} Usando el tercer método, resolver $y''-6y'+9y=25 \: e^x\: \sin x$
    $$\underbrace{(D^2-6D+9)}_{P(D)}y=\underbrace{25 \: e^x\: \sin x}_{b(x)}  $$
\end{ejer}
\begin{sol}
    Luego, por como es $b(x)$, $b(x) \in \ker ((D-1)^2+1^2)$, luego $Q(D)=D^2-2D+2$, que es primo con $P$, haciendo el algoritmo de Euclídes
    $$P(D)=Q(D) \cdot 1 + (-4D+7) \qquad Q(D)=(-4D+7)\left( -\dfrac{1}{4}D+\dfrac{1}{16}\right)+\dfrac{25}{16} \; \Rightarrow $$
    $$\dfrac{25}{16}=Q(D)+\underbrace{(4D-7)}_{P(D)-Q(D)}\left( -\dfrac{1}{4}D+\dfrac{1}{16}\right)=Q(D)\left(1-\dfrac{1}{4}D+\dfrac{1}{16} \right) + P(D)\left( -\dfrac{1}{4}D+\dfrac{1}{16}\right) \;\Rightarrow$$
    $$1=\dfrac{\left(1-\frac{1}{4}D+\frac{1}{16} \right)}{\nicefrac{25}{16}} \: Q(D)+\dfrac{\dfrac{1}{4}D+\frac{1}{16}}{\nicefrac{25}{16}} \: P(D)=1 \; \Rightarrow$$
    $$y_p=\dfrac{-\frac{1}{4}D-\frac{1}{16}}{\nicefrac{\cancel{25}}{16}} \: \cancel{25} \: e^x \sin x=(-4D-1) \: e^x \sin x$$
\end{sol}
\section{Ecuaciones de Euler}
Son de la forma
$$\sum_k a_k\:  x^k \: y^{(k}=0$$
es decir, que la potencia de $x$ es el orden de la derivada de $y$ a la que acompaña. Se realiza un cambio de variable $x=e^t$ (si $x<0$, ponemos $x=-e^t$), y procedemos con la siguiete notación:
$$\left\{ \begin{array}{l}
     f'=\dfrac{df}{dx}  \\
     \vspace{-5mm} \\
     \dot{f}=\dfrac{df}{dt} 
\end{array}\right.$$
y por la regla de la cadena
$$y'=\dfrac{dy}{dx}=\dfrac{\nicefrac{dy}{dt}}{\nicefrac{dx}{dt}}=\dfrac{\dot{y}}{x} \; \Rightarrow \; xy'=\dot{y}$$
De la misma forma, 
$$y''=\dfrac{d(\nicefrac{\dot{y}}{x})}{dx}=\dfrac{\frac{d\dot{y}}{dx} \: x - \dot{y}}{x^2}\overset{(1)}{=}\dfrac{\ddot{y}-\dot{y}}{x^2} \; \Rightarrow \; x^2 y''=\ddot{y}-\dot{y}$$
$^{(1)}$ por la regla de la cadena, $\dfrac{d\dot{y}}{dx}=\dfrac{\nicefrac{d\dot{y}}{dt}}{\nicefrac{dx}{dt}}=\dfrac{\ddot{y}}{x}$
y así sucesivamente.
\begin{ejer}
    \textbf{46.a)} $$0=x^2y''+2xy'-6y=\ddot{y}-\dot{y}+2\dot{y}-6y=\ddot{y}+\dot{y}-6y=(D^2+D-6)y=(D-2)(D-3)y$$
    luego las soluciones son
    $$y=c_1 \:  e^{2t} + c_2 \: e^{-3t}=c_1 \: x^2 + c_2 \: \dfrac{1}{x^3}$$
\end{ejer}
\section{Ecuación lineal asociada a un sistema de soluciones}
\begin{ejer}
    \textbf{47.} Encontrar la ecuación lineal homogénea para la que $e^x$, $e^{-x}$ forman una base del espacio de soluciones.
\end{ejer}
\begin{sol}
    Hacemos
    $$0=\left|\begin{array}{ccc}
         y &  e^x & e^{-x}\\
         y' &  e^x & -e^{-x} \\
         y'' & e^x & e^{-x}
    \end{array}\right|=\left| \begin{array}{cc}
        e^x & e^{-x} \\
         e^x & -e^{-x} 
    \end{array}\right|y''-\left| \begin{array}{cc}
        e^x & e^{-x} \\
         e^x & e^{-x} 
    \end{array}\right|y'+\left| \begin{array}{cc}
        e^x & -e^{-x} \\
         e^x & e^{-x} 
    \end{array}\right|y=-2y''+2y \; \iff \; \boxed{\: y''-y=0 \:}$$
    obteniendo una ecuación lineal homogénea que tiene a $e^x$ y $e^{-x}$ como soluciones.
\end{sol}
\section{Reducción del orden de una ecuación diferencial lineal conocida una solución}
Veamos el ejemplo para una de orden 2, sea
$$y''+a_1(x)y'+a_o(x)y=0$$
y sea $y_1$ una solución, haremos el cambio de variable $y=y_1\cdot z$, luego 
$$y'=y'_1 \cdot z + y_1 \cdot z' \qquad y'' = y''_1 \cdot z + 2y'_1 z' + y_1 \cdot z'' $$
y sutituyendo en la ecuación inicial:
$$0=y''_1 z +2y'_1z'+y_1z''+a_1(x)(y'_1z+y_1z')+a_o(x) y_1 z=z\cancelto{0}{(y_1''+a_1(x)y'_1+a_o(x)y_1)}+2y'_1z'+y_1z''+a_1(x)y_1z'$$
$$0=y_1z''+(2y_1'+a_1(x)y_1)z'$$
y dado que solo tenemos $z'$ y $z''$, hacemos el cambio $u:=z'$, luego
$$0=y_1u'+(2y_1'+a_1(x)y_1)u$$
que ya es de 1er orden.
\begin{eje} \textbf{48.a)}
    Integrar la ecuación $y''+\dfrac{2}{x}y'+y=0$ sabiendo que $y_1=\frac{\sin x}{x}$ es solución
\end{eje}
\begin{sol}
    Cambio $y=y_1 \: z$,
    $$0=y_1 \: u'+\left(2y_1'+\dfrac{2}{x}\right)u \; \iff \; 0=u'+\left(\dfrac{2y_1'}{y_1}+\dfrac{2}{x}\right)u \; \iff \; 0=\dfrac{du}{u}+\left(\dfrac{2y_1'}{y_1}+\dfrac{2}{x}\right)dx=d\left(\log|u|+2 \log|y_1 \cdot x|\right)$$
    luego una solución es
    $$u(y_1 x)^2=c_1 \; \Rightarrow \; u=\dfrac{c_1}{(y_1 x)^2}=\dfrac{c_1}{\sin^2 x}$$
    y deshaciendo el cambio,
    $$z'=u=\dfrac{c_1}{\sin^2 x} \; \Rightarrow \; z=-c_1 \dfrac{\cos x }{\sin x}+c_2 \; \Rightarrow \; y=y_1 z =\dfrac{\sin x }{x} z =-c_1 \dfrac{\cos x }{x}+c_2 \dfrac{\sin x }{x}$$
\end{sol}
\section{Series de potencias}
\begin{eje}
Sea la ecuación 
$$y''-xy'+y-1=0$$
hallar soluciones con series de potencias centradas en $x_o=0$, esto es 
$$y=a_o+a_1x+a_2x^2+a_3x^3+\ldots=\sum_{n=0}^{\infty} a_n \: x^n$$ 
con $a_i$ indeterminadas (todavía). Derivando,
$$y'=a_1+2a_2x+3a_3x^2+ \ldots = \sum_{n=1}^{\infty} n \: a_n \: x^{n-1}$$
$$xy'=\sum_{n=1}^{\infty} n \: a_n \: x^{n}$$
$$y''=\sum_{n=2}^{\infty} n(n-1) \: a_n \: x^{n-2}$$
de forma que la ecuación reescrita es
\begin{equation}
    0=\sum_{n=2}^{\infty} n(n-1) \: a_n \: x^{n-2} - \sum_{n=1}^{\infty} n \: a_n \: x^{n}+\sum_{n=0}^{\infty} a_n \: x^n -1 
\end{equation}
ahora bien, haciendo $k=n-2$, 
$$0=\sum_{k=0}^{\infty} (k+2)(k+1) \: a_{k+2} \: x^{k} \; \iff \; 0=\sum_{n=0}^{\infty} (n+2)(n+1) \: a_{n+2} \: x^{n}$$
(lo cual es posible simplemente cambiándole el nombre). Volviendo a la ecuación (3.1), extraemos los coeficientes en $n=0$ de los sumatorios
$$0=2a_2+a_o-1+\mathlarger{\sum_{n=1}^{\infty}} ((n+2)(n+1) \: a_{n+1} - n \: a_n + a_n) \: x^n \; \iff \;$$ $$ \begin{cases} 2a_2+a_o-1=0 \\ (n+2)(n+1) \: a_{n+1} - n \: a_n + a_n = 0 \qquad  n \geq 1 
\end{cases} \; \Rightarrow \; \begin{cases}
    a_2=\dfrac{1}{2}-\dfrac{1}{2} a_o \\ a_{n+2}=\dfrac{n-1}{(n+2)(n+1)}a_n 
\end{cases}$$ 
y podemos ir bajando en la sucesión, haciendo $m:=n+2$, $$a_m=\dfrac{m-3}{m(m-1)}a_{m-2}=\dfrac{m-3}{m(m-1)} \cdot \dfrac{m-5}{(m-2)(m-3)}a_{m-4}= \cdots$$
Dado que la regla de recurrencia va de dos en dos, vamos a distinguir dos casos:

\underline{Caso $m=2k$}
$$a_{2k}=\underbrace{\dfrac{2k-3}{2k(2k-1)} \cdot \dfrac{2k-5}{(2k-2)(2k-3)} \cdot \dfrac{2k-7}{(2k-4)(2k-5)} \cdot \cdots \dfrac{3}{6 \cdot 5} \cdot \dfrac{1}{4 \cdot 3}}_{\lambda_k} a_2= -\dfrac{\lambda_k}{2}a_o+\dfrac{\lambda_k}{2}$$
Además, podemos abreviar $\lambda_k$ como sigue:
$$\lambda_k=\dfrac{\nicefrac{(2k-3)!}{(k-2)!2^{k+2}}}{\nicefrac{(2k)!}{2}}=\dfrac{(2k-3)!}{2^{k-3}(2k)!(k-2)!}$$

\underline{Caso $m=2k+1$} ($k \geq 1$)
$$a_{2k+1}=\dfrac{2k-2}{(2k+1)2k}\: a_{2k-1}=\dfrac{2k-2}{(2k+1)2k}\cdot \dfrac{2k-4}{(2k-1)(2k-2)}\: a_{2k-3}= \: \cdots \:  = \dfrac{(2k-2)(2k-4) \cdots 4 \cdot 2}{(2k+1)(2k)(2k-1) \: \cdots \: 3 \cdot 2} \:\cancelto{0}{a_3}=0$$

Y por tanto, la solución es $$y=a_o+a_1\cdot x+\sum_{k=1}^{\infty}\left(-\dfrac{1}{2}a_o+\dfrac{1}{2} \right) \lambda_k \cdot x^{2k}=a_o\left( 1+\sum_{k=1}^{\infty} -\dfrac{1}{2} \lambda_k \cdot x^{2k} \right) + a_1 \cdot x + \sum_{k=1}^{\infty} -\dfrac{1}{2} \: \lambda_k \: \cdot x^{2k}$$
con $a_o,a_1 \in \mathbb R$ arbitrarias. Comprobemos la convergecia,
$$\sum_{k=1}^{\infty} \lambda_k \cdot x^{2k}=\sum_{k=1}^{\infty} \lambda_k \cdot (x^2)^k \; \Rightarrow \; \lim_{k \to \infty} \dfrac{\lambda_{k+1}}{\lambda_k}= \lim_{k \to \infty} \dfrac{2k-1}{(2k+2)(2k+1)}=0=L \; \Rightarrow \; R=+\infty$$
es decir, la serie converge para todo $x \in \mathbb R$. En realidad, hemos realizado todas las operaciones sin saber si podíamos hacerlo realmente, pero hemos comprobado que converge, luego podemos garantizar que es solución. 
\end{eje}

\subsection{Ecuaciones de Bessel}
\begin{defi}
    Se dice que un punto $x_o$ es singular regular de $y''+A(x)y'+B(x)y=0$ si 
    $$\left\{ \begin{array}{ll}
         (x-x_o)A(x)=:P(x) \text{ analítica }  & (A=\frac{P(x)}{x-x_o}) \\
         (x-x_o)^2 B(x)=:Q(x) \text{ analítica } & (B=\frac{Q(x)}{(x-x_o)^2})
    \end{array}\right.$$
    En ese caso, hay una solución del tipo 
    $$y=(x-x_o)^r \: \sum_{n=0}^{\infty} a_{n}(x-x_o)^n$$
    para cierto $r \in \mathbb R$.
\end{defi}
\begin{eje}[\textbf{Ecuación de Bessel}]
    $$y''+\dfrac{1}{x}y'+\dfrac{x^2-\alpha^2}{x^2}y=0 \quad \alpha \in \mathbb R$$
    $x_o=0 $
    $$A(x)=\dfrac{1}{x};\; \; x \: A(x)=1 \qquad B(x)=\dfrac{x^2-\alpha^2}{x^2};\; \; x^2 \: B(x)=x^2-\alpha^2$$
    Buscamos soluciones
    $$y=x^r \sum_{n=0}^{\infty} a_n \: x^n=\sum_{n=0}^{\infty} a_n \: x^{n+r}$$
    y derivando
    $$y'=r \: a_o \:x^{r-1}+(r+1) \: a_1 \:x^{r}+(r+2) \: a_2 \:x^{r+1} + \cdots=\sum_{n=0}^{\infty} (n+r)a_n x^{n+r-1}$$
    $$y''=r(r-1) \: a_o \:x^{r-2}+(r+1)r \: a_1 \:x^{r-1}+(r+2)(r+1) \: a_2 \:x^{r} + \cdots=\sum_{n=0}^{\infty} (n+r-1)(n+r)a_n x^{n+r-2}$$
    y la ecuación la podemos reescribir como
    $$0=x^2y''+xy'+(x^2-\alpha^2)y=\sum_{n=0}^{\infty} (n+r-1)(n+r)a_n x^{n+r} + \sum_{n=0}^{\infty} (n+r)a_n x^{n+r} + \sum_{n=0}^{\infty} a_n \: x^{n+r+2}-\alpha^2\sum_{n=0}^{\infty} a_n \: x^{n+r}=$$
    $$=(r(r-1)+r-\alpha^2)a_ox^r + (r(r+1)a_1+(r+1)a_1-\alpha^2a_1)x^{r+1}+((r+2)(r+1)a_2+(r+2)a_2+a_o-\alpha^2a_o)x^{r+2}+\cdots =$$
    $$=(r^2-\alpha^2)a_o x^r + ((r+1)^2-\alpha^2)a_1x^{r+1}+((r+2)^2-\alpha^2)a_2+a_o+\cdots$$
    pudiendo resolver como sistema:
    $$\left\{ \begin{array}{l}
         0=(r^2-\alpha^2)a_o \overset{a_o\neq 0}{\Rightarrow} r^2-\alpha^2=0 \iff r=\pm \alpha \text{ sea } r=\alpha\\
         0=((r+1)^2-\alpha^2)a_1 = ((\alpha+1)^2-\alpha^2)a_1 \iff a_1=0 \\
         0=((r+2)^2-\alpha^2)a_2+a_o \\
         0=((r+3)^2-\alpha^2)a_3+a_1 \\
         \vdots \\
         0=((r+n)^2-\alpha^2)a_n+a_{n-2} \; \; \;n \geq 2
    \end{array}\right.$$
    teniendo una ecuación general
    $$a_n=\dfrac{-1}{((\alpha+n)^2-\alpha^2)}a_{n-2}=\dfrac{-1}{n(2\alpha+n)}a_{n-2}$$
    con $a_o$ arbitrario y $a_1=0$. Entonces, para $n \geq 2$,
    $$a_n=\dfrac{-1}{n(2\alpha+n)} \cdot \dfrac{-1}{(n-2)(2\alpha+n-2)}a_{n-4}=\cdots $$
    Luego sepraramos los casos par e impar:

    Si $n=2m+1$
$$a_{2m+1}=0$$
    
    Si $n=2m$
    $$a_{2m}=\dfrac{-1}{2m(2\alpha+2m)}a_{2(m-1)}=\dfrac{-1}{2^2m(\alpha+m)}a_{2(m-1)}=\dfrac{-1}{2^2m(\alpha+m)} \cdot \dfrac{-1}{2^2(m-1)(\alpha+m-1)}a_{2(m-2)}=\cdots=$$
    de forma que 
    $$a_{2m}=\dfrac{(-1)^m}{2^{2m} m! (\alpha+m) (\alpha+m-1) (\alpha+m-2) \cdots (\alpha+1)\alpha}a_o=\mu_m \: a_o$$
    y por tanto la solución es 
    $$\boxed{ \: y=a_o \sum_{m=0}^{\infty} \mu_m \: x^{2m+\alpha} \: }$$

    Dada la importancia de esta solución, vamos a elegir una base del espacio. Consideremos la función 
    $$\Gamma(p)=\int_0^{\infty}e^{-t} \: t^{p-1} \: dt$$
    que verifica que $\Gamma(p+1)=p \: \Gamma(p)$. Entonces, para $a_o$ elegimos
    $$a_o=\dfrac{1}{2^{\alpha} \: \Gamma(\alpha+1)} $$
    de forma que 
    $$a_o\mu_m=\dfrac{1}{2^{\alpha} \: \Gamma(\alpha+1)}  \cdot \dfrac{(-1)^m}{2^{2m} m! (\alpha+m) (\alpha+m-1) (\alpha+m-2) \cdots (\alpha+1)\alpha}=\dfrac{-1}{m! \Gamma(\alpha+m+1)} \cdot \dfrac{1}{2^{2m+\alpha}} $$
    por lo que 
    $$y=\sum_{m=0}^{\infty} \dfrac{(-1)^m}{m! \Gamma(\alpha+m+1)} \left(\dfrac{x}{2}\right)^{2m+\alpha}=J_{\alpha}$$
    donde $J_{\alpha}$ se denomina función de Bessel de primera especie y orden $\alpha$.

    Si $\alpha\nin \mathbb N$, se puede generar con $-\alpha$ y son linealmente independientes. 
    
    Si $\alpha\in \mathbb N$,  $J_{-\alpha}$ es linealmente dependiente de $J_{\alpha}$. Se puede encontrar otra solución $Y_{\alpha}$ y se denomina de Bessel de segunda especie.
\end{eje}
\chapter{Tipos y métodos de resolución de ecuaciones en derivadas parciales}
\section{Ecuaciones lineales}
Sean ahora $x,y$ variables independientes y sea $z=z(x,y)$ la variable incógnita, son ecuaciones (en $\mathbb R^3$) del tipo
$$A(x,y) \pd{z}{x} + B(x,y) \pd{z}{y}=C(x,y,z)$$

El campo (en $\mathbb R^2_{x,y}$) asociado a esta ecuación es 
$$D=A(x,y)\pd{}{x}+B(x,y)\pd{}{y}$$
luego para resolver la ecuación, basta resolver
$$Dz=C(x,y,z)$$

Ahora bien, por lo visto en la teoría, el campo $D$ puede reducirse a forma canónica, por un cambio de coordenadas $(r,\lambda)$, siendo $r:=x$ y $\lambda$ una integral 1ª, verificaría que
$$D=Dr \: \pd{}{r}+ D\lambda \: \pd{}{\lambda}=Dx \: \pd{}{r}=A(x,y) \: \pd{}{x}$$
y definiendo 
$$\mu:= \int \dfrac{1}{A(x,y)} dx$$
el campo $D$ queda completamente reducido, al cambiar a las coordenadas $(\mu, \lambda)$
$$D=\pd{}{\mu}$$
por lo que 
$$Dz=C(x,y,z) \iff \pd{z}{\mu}=C(x,y,z) $$
resolviéndose por cuadraturas.

\begin{eje}
        En el caso de la ecuación:
        $$z \pd{z}{x}+x\pd{z}{y}=xz$$ 
        siendo $z=z(x,y)$ incógnita y $x,y$ variables independientes, no podemos usar el método que solíamos usar, ya que para resolverlas hacemos
        $$Dz=xz \text{ con } D=z\pd{}{x} + x \pd{}{y}$$
        y el campo $D$ no es un campo en $\mathbb R^2_{x,y}$, teniendo que definir el siguiente tipo de ecuaciones:
    \end{eje}
\section{Ecuaciones cuasilineales}

Son del tipo
    $$P(x,y,z) \pd{z}{x}+Q(x,y,z) \pd{z}{y}=R(x,y,z) \; (*)$$
    \subsection{Método de resolución para ecuaciones cuasilineales}

    En primer lugar, consideramos el campo:
    $$D:=P(x,y,z) \pd{}{x}+Q(x,y,z) \pd{}{y}+R(x,y,z) \pd{}{z}$$
    y sea $u:=u(x,y,z)$ una integral 1ª de $D$ ($0=Du=P\pd{u}{x}+Q\pd{u}{y}+R\pd{u}{z}$). 

    En la ecuación $u(x,y,z)=0$ (como superficie), suponemos que $z=\psi(x,y)$ implícitamente. 
    \begin{prop}
        Entonces, $z=\psi(x,y)$ es solución de $(*)$.
    \end{prop}
    \begin{dem}
        $$0=u(x,y,\psi(x,y)) \overset{\pd{}{x}}{\longrightarrow} 0=\pd{u}{x}(x,y,\psi) + \pd{u}{z} (x,y,\psi) \cdot \pd{\psi}{x}(x,y)$$
        $$0=u(x,y,\psi(x,y)) \overset{\pd{}{y}}{\longrightarrow} 0=\pd{u}{y}(x,y,\psi) + \pd{u}{z} (x,y,\psi) \cdot \pd{\psi}{y}(x,y)$$
        (se ha usado la regla de la cadena al derivar). Despejamos $\pd{u}{x}$ y $\pd{u}{y}$, y al sustituirlas en el campo $D$:
        $$0=P\left(-\pd{u}{z} \cdot \pd{\psi}{x} \right) + Q\left(-\pd{u}{z} \cdot \pd{\psi}{y} \right) + R \pd{u}{z} \; \Huge|_{z=\psi(x,y)}=\pd{u}{z}\cdot\left(-P \cdot \pd{\psi}{y} -q \cdot \pd{\psi}{y} + R  \right) \; \Huge|_{z=\psi(x,y)} $$
        de forma que 
        $$\left\{ \begin{array}{l}
             \pd{u}{z}=0 \: (\text{no es 0 ya que si no no podríamos depsejar})  \\
             \text{ ó } \\
             -P \cdot \pd{\psi}{y} -q \cdot \pd{\psi}{y} + R =0 \iff \; P(x,y,\psi) \pd{\psi}{x}+Q(x,y,\psi) \pd{\psi}{y}=R(x,y,\psi)
        \end{array} \right.$$
            es decir, $\psi(x,y)$ es una solución de $(*)$.
    \end{dem}
    \begin{eje}
        $$z\pd{z}{x}+x\pd{z}{y}=xz$$
        Consideramos el campo
        $$D=z \pd{}{x} + x \pd{}{y} + xz \pd{}{z}$$
        y buscamos sus integrales $1^{as}$ de $D$, que como hemos calculado son
        $$\left\{ \begin{array}{l}
             \lambda:=z e^{-y}  \\
             \mu:= z- \dfrac{x^2}{2} 
        \end{array} \right.$$
        que además son funcionalmente independientes, y no hay más. Es decir, que toda integral 1ª de $D$ es de la forma $u=\gamma(\lambda,\mu)=\gamma(z e^{-y}, z- \nicefrac{x^2}{2} )$
        por ejemplo, son integrales $1^{as}$:
        $$(ze^{-y})^2 \cos(z-\nicefrac{x^2}{2}) \quad \log(1+ze^{-y})^2+(z-\nicefrac{x^2}{2}) ^2) + ze^{-y} $$

        Entonces, si de la ecuación 
        $$0=u=\gamma(z e^{-y}, z- \nicefrac{x^2}{2})$$
        podemos despjear $z=\psi(x,y)$, tendremos una solución.

        Si consideramos 
        $$0=2ze^{-y} + 1 + z -\nicefrac{x^2}{2}=z(2e^{-y}+1)+1-\nicefrac{x^2}{2} \; \Rightarrow \; z=\dfrac{\nicefrac{x^2}{2}-1}{2e^{-y}+1}$$
        y otro ejemplo,
        $$0=(ze^{-y})^2 + z + \nicefrac{x^2}{2} \; \Rightarrow \; z=\dfrac{-1\pm \sqrt{1+2x^2e^{-2y}}}{2e^{-y}}$$
    \end{eje}
    \begin{obs}
        Intentemos que verifique una condición, sea la ecuación anterior, pasando por la curva $x=y=z$.

        Sabemos que las soluciones las encontramos con la ecuación
        $$0=u=\gamma(\lambda,\mu)=\gamma(z e^{-y}, z- \nicefrac{x^2}{2} )$$
        es decir, tenemos que encontrar $\gamma$ tal que al despejar $z(x,y)$, sea una superficie que pase por $
            x=t, \; y=t, \;  z=t$. Entonces
            $$0=\gamma(t e^{-t}, t- \nicefrac{t^2}{2})$$
            si consideramos $s=t -\nicefrac{t^2}{2}$, luego $t=1\pm \sqrt{1-2s}$(siendo $s \leq \nicefrac{1}{2}$). En el caso positivo, 
            $$0=\gamma((1\pm \sqrt{1-2s})e^{-(1\pm \sqrt{1-2s})},s)$$
            entonces, definiendo $\gamma$ como 
            $$\gamma(p,q):= p - (1\pm \sqrt{1-2q})e^{-(1\pm \sqrt{1-2q})} $$
            se verifica la ecuación, de forma que la solución a la ecuación pasando por $x=y=z$ es 
            $$0=\gamma(\lambda,\mu)=\lambda- (1\pm \sqrt{1-2\mu})e^{-(1\pm \sqrt{1-2\mu})}=ze^{-y} - (1+\sqrt{1-2(z- \nicefrac{x^2}{2})} e^{-(1+\sqrt{1-2(z- \nicefrac{x^2}{2})})}$$
            y despejando de ahí la variable $z$, obtenemos la solución explícita.
    \end{obs}
    \begin{obs}
        Dado que la condición $\gamma(\lambda,\mu)=0$ equivale que a 
        $$\left\{ \begin{array}{l}
             \mu=\alpha(\lambda)  \\
             \text{ ó } \\
             \lambda=\beta(\mu) 
        \end{array} \right.$$
        de ahí despejamos más sencillamente.
    \end{obs}
    \begin{ejer}
        Calcular una superficie solución de 
        $$y \pd{z}{x} + yz^2 \pd{z}{y}=z^3$$
        que pase por la curva $x=t, \: y=t, \: z=\nicefrac{1}{t^6}$. 
    \end{ejer}
    \begin{sol}
        La ecuación tiene el campo asociado
        $$D=y \pd{}{x} + yz^2 \pd{}{y}+z^3 \pd{}{z} \iff \dfrac{dx}{y}=\dfrac{dy}{yz^2}=\dfrac{dz}{z^3}$$
        $$\dfrac{dy}{y}=\dfrac{dz}{z} \; \Rightarrow \; \lambda:= \nicefrac{y}{z}$$
        y podemos considerar $y=\lambda z$ con $\lambda$ como constante, que en verdad no lo es, pero en las solucione sí lo es.
        $$\dfrac{dx}{y}=\dfrac{dz}{z^3} \Rightarrow \dfrac{dx}{\lambda z}=\dfrac{dz}{z^3} \Rightarrow \dfrac{dx}{\lambda}=\dfrac{dz}{z^2} \; \Rightarrow \; 0 =dx-\lambda \dfrac{dz}{z^2}=d(x+\nicefrac{\lambda}{z})$$
        Por eso, $x+\nicefrac{\lambda}{z}=x+\nicefrac{y}{z^2}$ es constante en las soluciones. Luego $\mu:=x+\nicefrac{y}{z^2}$ es otra integral 1ª. 

        Las superficies solución de la ecuación en derivadas parciales planteada son:
        $$\psi(\lambda,\mu)=0 \text{ ó } \lambda=\alpha(\mu) \text{ ó } \mu=\beta(\lambda)$$
        Probemos suerte con $\lambda=\alpha(\mu)$, es decir, hacer $\dfrac{y}{z}=\alpha\left(x+\dfrac{y}{z^2}\right)$ y determinamos $\alpha$ para que pase por la curva enunciada:
        $$\dfrac{t}{\nicefrac{1}{t^6}}=\alpha\left( t+\dfrac{t}{\nicefrac{1}{t^{12}}} \right) \iff t^7=\alpha(t+t^{13})$$
        lo cual no se puede resolver fácilmente. 

        Probemos con $\mu=\beta(\lambda) \Rightarrow x+\nicefrac{y}{z^2}=\beta(\nicefrac{y}{z})$ como antes:
        $$t+\dfrac{t}{\nicefrac{1}{t^{12}}}=\beta\left( \dfrac{t}{\nicefrac{1}{t^{6}}} \right) \Rightarrow t+t^{13}=\beta(t^7) $$
        y haciendo $s:=t^7$, $t=\sqrt[7]{s}$, luego $\beta(s)=s^{\nicefrac{1}{7}}+s^{\nicefrac{13}{7}}$. Sustituyendo en la solución buscada,
        $$x+\dfrac{y}{z^2}=\left(\dfrac{y}{z}\right)^{\nicefrac{1}{7}}+\left(\dfrac{y}{z}\right)^{\nicefrac{13}{7}}$$
    \end{sol}
    
\begin{obs}
Combinaciones integrables.

Dados los cocientes
$$\dfrac{a}{b}=\dfrac{c}{d}=\dfrac{ra+sc}{rb+sd} \qquad r,s \in \mathbb R$$
así pues también se verifica para funciones, de forma que
$$\dfrac{dx}{f}=\dfrac{dy}{g}=\dfrac{dz}{h}=\dfrac{rdx+sdy+tdz}{rf+sg+th} \qquad r,s,t \text{  funciones }$$
de forma que si $rdx+sdy+tdz=d\lambda$ es un potencial y $rf+sg+th=0$, entonces $\lambda$ es integral 1ª. 
\end{obs}
   